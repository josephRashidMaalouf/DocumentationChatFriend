# DocumentationChatFriend

Ett lokalt RAG (Retrieval-Augmented Generation) API byggt med ASP.NET Core (.NET 9) och Clean Architecture. Allt du beh√∂ver √§r Docker ‚Äì inga ytterligare beroenden eller konfigurationer kr√§vs lokalt.

## üöÄ Kom ig√•ng

### F√∂rkrav
- [Docker](https://www.docker.com/) installerat

### Starta projektet
Ladda ner docker-compose filen, och √∂ppna en terminal i samma mapp d√§r filen finns.
Skriv in kommandot:
```bash
docker compose up
```
Detta startar API:t tillsammans med Ollama och Qdrant, och laddar automatiskt ned alla n√∂dv√§ndiga modeller fr√•n Docker Hub. I terminalen kommer du se hur n√∂dv√§ndiga Ollama modeller laddas ned. N√§r nedladdningen √§r f√§rdig startar API:et automatiskt ig√•ng.

### Modellkonfiguration

Du kan konfigurera vilka modeller som anv√§nds direkt i docker-compose.yml genom att s√§tta milj√∂variabler:

I docker-compose filen ser det ut s√• h√§r:
```
  documentation-chat-friend-backend:
    image: josephrashidmaalouf/documentation-chat-friend-backend:latest
    container_name: "documentation-chat-friend-backend"
    ports:
      - "5143:5143"
    environment:
      - DOTNET_ENVIRONMENT=Docker
      - OllamaModelConfigs__Models__0=gemma3:1b 
      - OllamaModelConfigs__Models__1=nomic-embed-text:latest # This line, and the one above will make sure these models are pulled to the ollama container
      - OllamaClientConfigs__Model=gemma3:1b # This sets the LLM to be used for formulating answers
      - OllamaClientConfigs__MaxTokens=512 # Configure max tokens allowed in response
      - OllamaClientConfigs__Temperature=0.9 # Configure temperature (creativity) in responses. A lower number means less creative
      - VectorRepositoryConfigs__MinScore=0.7 # Configure the accuracy score on the embedding retrieved from the database in relation to the question asked
      - VectorRepositoryConfigs__Limit=3 # Configure maximum allowed embeddings to be retrieved from the database
    pull_policy: always

```
- OllamaClientConfigs__Model=gemma3:1b 
P√• den raden kan du byta ut v√§rdet till valfri lokal Ollama model. Men se d√• till att den laddas ned genom att l√§gga till en ny rad f√∂r Models:
`- OllamaModelConfigs__Models__3=ny-model `
Eller byta ut Models p√• index 0 till den modell du vill anv√§nda.

F√∂r en lista p√• Ollama modeller h√§nvisar jag till deras dokumentation: [https://ollama.com/library](https://ollama.com/library)
Se till att inte anv√§nda en starkare modell √§n vad din maskin klarar av. Tar modellen med √§n 100 sekunder p√• sig att formulera ett svar s√• kommer API:t inte svara.

Just nu √§r embedding modellen inte konfigurerbar, men det kommer i n√§sta version.

H√§r √§r teknikstack-sektionen i kopierbar `.md`-format:

## Teknikstack och arkitektur

* .NET 9 (ASP.NET Core)
* Clean Architecture
* Ollama (f√∂r b√•de embedding och LLM-svar)
* Qdrant (vektordatabas)
* Docker Compose

## Nuvarande funktioner

* Tv√• endpoints:

  * `POST /api/upload` f√∂r att ladda in och embeda text
  * `POST /api/completions` f√∂r att st√§lla fr√•gor och f√• svar baserat p√• embedda data
* Anv√§nder Ollama embedding modeller (default: `nomic-embed-text`)
* Lagrar embeddingar i Qdrant
* Anv√§nder Ollama LLM (default: `gemma3:1b`) f√∂r att generera svar baserat p√• relevant fakta
* Inga hallucinationer ‚Äì svarar "Jag vet inte" om ingen fakta hittas
* Docker Compose f√∂r enkel lokal k√∂rning utan extra konfiguration

## Planerade features

* Konfigurerbar embedding modell
* Endpoint f√∂r chatt med kontext/minne
* Streaming av svar fr√•n LLM
* M√∂jlighet att l√§sa in `.txt` och `.pdf`-filer direkt
* Frontend-gr√§nssnitt

# API Documentation

## Endpoints

### POST `/api/completions`

Submit a question to the RAG (Retrieval-Augmented Generation) service and receive a generated answer.

**Request Body**

```
{
    "question" : "string",
    "collectionName" : "string"
}
```

- `question` (string, required): The question to be answered.
- `collectionName` (string, required): The name of the collection to query facts from.

**Responses**

- `200 OK`: Returns the generated answer as a string.
- `404 Not Found`: The specified collection or data was not found.
- `503 Service Unavailable`: The service is temporarily unavailable or an error occurred.

---

### POST `/api/upload`

Upload text to a collection, chunked according to the specified style, for later querying.

**Request Body**

```
{
    "collectionName" : "string,
    "text" : string,
    "chunkingStyle" 0|1|2
}
```

- `collectionName` (string, required): The name of the collection to upload to.
- `text` (string, required): The text content to be chunked and uploaded.
- `chunkingStyle` (enum, required): The chunking style. Possible values: 0 = `Sentence`, 1 = `Paragraph`, 2 = `Custom`.

**Query Parameters**

- `chunkLength` (int, optional, default: 10): (Custom only) Length of each chunk.
- `overlap` (int, optional, default: 0): (Custom only) Overlap between chunks.

Will only be used with the Custom (2) chunkingStyle

**Responses**

- `200 OK`: Upload and chunking succeeded.
- `503 Service Unavailable`: The service is temporarily unavailable or an error occurred.

---
