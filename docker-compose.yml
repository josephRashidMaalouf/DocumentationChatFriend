services:
  documentation-chat-friend-backend:
    image: josephrashidmaalouf/documentation-chat-friend-backend:latest
    container_name: "documentation-chat-friend-backend"
    ports:
      - "5143:5143"
    environment:
      - DOTNET_ENVIRONMENT=Docker
      - OllamaModelConfigs__Models__0=gemma3:1b 
      - OllamaModelConfigs__Models__1=nomic-embed-text:latest # This line, and the one above will make sure these models are pulled to the ollama container
      - OllamaClientConfigs__LLMModel=gemma3:1b # This sets the LLM to be used for formulating answers
      - OllamaClientConfigs__EmbeddingModel=nomic-embed-text # This sets the embedding model to be used for formulating answers
      - OllamaClientConfigs__MaxTokens=512 # Configure max tokens allowed in response
      - OllamaClientConfigs__Temperature=0.9 # Configure temperature (creativity) in responses. A lower number means less creative
      - VectorRepositoryConfigs__MinScore=0.7 # Configure the accuracy score on the embedding retrieved from the database in relation to the question asked
      - VectorRepositoryConfigs__Limit=3 # Configure maximum allowed embeddings to be retrieved from the database
    pull_policy: always

  ollama:
    image: ollama/ollama:latest
    container_name: "ollama"
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama/models
    pull_policy: always

  qdrant:
    image: qdrant/qdrant:latest
    container_name: "qdrant"
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage

volumes:
  ollama_data:
  qdrant_data: